{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Part 2: Training the Delta Observer\n",
    "\n",
    "This notebook trains the **Delta Observer** network that learns to map between monolithic and compositional representations.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Monolithic (64D) ‚îÄ‚îÄ‚Üí Encoder ‚îÄ‚îÄ‚îê\n",
    "                                ‚îú‚îÄ‚îÄ‚Üí Shared Latent (16D) ‚îÄ‚îÄ‚Üí Decoders\n",
    "Compositional (64D) ‚îÄ‚Üí Encoder ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "The 16D latent space learns the **semantic primitive** that distinguishes these representations.\n",
    "\n",
    "üìÑ **Paper:** [OSF MetaArXiv](https://doi.org/10.17605/OSF.IO/CNJTP)  \n",
    "üîó **Code:** [github.com/EntroMorphic/delta-observer](https://github.com/EntroMorphic/delta-observer)\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** For the complete **Online Delta Observer** pipeline (recommended), use **`99_full_reproduction.ipynb`** which trains all models concurrently and captures the transient clustering phenomenon. This notebook demonstrates the post-hoc training approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (Colab)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_needed(package):\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "install_if_needed('torch')\n",
    "install_if_needed('matplotlib')\n",
    "install_if_needed('scikit-learn')\n",
    "\n",
    "print('‚úÖ Dependencies ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üñ•Ô∏è Using device: {device}')\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    'train': '#3498db',\n",
    "    'val': '#e74c3c',\n",
    "    'accent': '#2ecc71'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìÇ Load Pre-computed Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository if running in Colab\n",
    "repo_dir = 'delta-observer'\n",
    "if not os.path.exists(repo_dir) and not os.path.exists('../data'):\n",
    "    print('üì• Cloning delta-observer repository...')\n",
    "    !git clone https://github.com/EntroMorphic/delta-observer.git\n",
    "    print('‚úÖ Repository cloned!')\n",
    "\n",
    "# Smart path detection\n",
    "possible_data_dirs = ['../data', 'data', 'delta-observer/data']\n",
    "data_dir = next((p for p in possible_data_dirs if os.path.exists(p)), None)\n",
    "\n",
    "possible_models_dirs = ['../models', 'models', 'delta-observer/models']\n",
    "models_dir = next((p for p in possible_models_dirs if os.path.exists(p)), '../models')\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "print(f'üìÅ Data directory: {data_dir}')\n",
    "print(f'üìÅ Models directory: {models_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load activations\n",
    "mono_data = np.load(os.path.join(data_dir, 'monolithic_activations.npz'))\n",
    "comp_data = np.load(os.path.join(data_dir, 'compositional_activations.npz'))\n",
    "\n",
    "mono_activations = mono_data['activations']\n",
    "comp_activations = comp_data['activations']\n",
    "inputs = mono_data['inputs']\n",
    "carry_counts = mono_data['carry_counts']\n",
    "\n",
    "print(f'üìä Monolithic activations: {mono_activations.shape}')\n",
    "print(f'üìä Compositional activations: {comp_activations.shape}')\n",
    "print(f'üìä Carry counts: {np.bincount(carry_counts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualize input activations\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, activations, name in [\n",
    "    (axes[0], mono_activations, 'Monolithic'),\n",
    "    (axes[1], comp_activations, 'Compositional')\n",
    "]:\n",
    "    pca = PCA(n_components=2)\n",
    "    act_2d = pca.fit_transform(activations)\n",
    "    scatter = ax.scatter(act_2d[:, 0], act_2d[:, 1], c=carry_counts, \n",
    "                         cmap='viridis', s=30, alpha=0.7)\n",
    "    ax.set_title(f'{name} Activations (PCA)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "\n",
    "plt.colorbar(scatter, ax=axes, label='Carry Count', shrink=0.8)\n",
    "plt.suptitle('üîç Input Activations to Delta Observer', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nüí° These are the representations the Delta Observer will learn to bridge!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaObserverDataset(Dataset):\n",
    "    def __init__(self, mono_act, comp_act, carry_counts, inputs):\n",
    "        self.mono_act = torch.tensor(mono_act, dtype=torch.float32)\n",
    "        self.comp_act = torch.tensor(comp_act, dtype=torch.float32)\n",
    "        self.carry_counts = torch.tensor(carry_counts, dtype=torch.long)\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mono_act)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'mono_act': self.mono_act[idx],\n",
    "            'comp_act': self.comp_act[idx],\n",
    "            'carry_count': self.carry_counts[idx],\n",
    "            'input': self.inputs[idx],\n",
    "        }\n",
    "\n",
    "dataset = DeltaObserverDataset(mono_activations, comp_activations, carry_counts, inputs)\n",
    "print(f'üìä Dataset size: {len(dataset)}')\n",
    "\n",
    "# Split 80/20\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f'üìä Train: {len(train_dataset)}, Val: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Delta Observer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaObserver(nn.Module):\n",
    "    def __init__(self, mono_dim=64, comp_dim=64, latent_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dual encoders\n",
    "        self.mono_encoder = nn.Sequential(\n",
    "            nn.Linear(mono_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.comp_encoder = nn.Sequential(\n",
    "            nn.Linear(comp_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # Shared latent encoder\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoders\n",
    "        self.mono_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, mono_dim),\n",
    "        )\n",
    "        \n",
    "        self.comp_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, comp_dim),\n",
    "        )\n",
    "        \n",
    "        # Carry count regressor\n",
    "        self.carry_regressor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, mono_act, comp_act):\n",
    "        mono_enc = self.mono_encoder(mono_act)\n",
    "        comp_enc = self.comp_encoder(comp_act)\n",
    "        joint = torch.cat([mono_enc, comp_enc], dim=-1)\n",
    "        return self.shared_encoder(joint)\n",
    "    \n",
    "    def forward(self, mono_act, comp_act):\n",
    "        latent = self.encode(mono_act, comp_act)\n",
    "        mono_recon = self.mono_decoder(latent)\n",
    "        comp_recon = self.comp_decoder(latent)\n",
    "        carry_pred = self.carry_regressor(latent)\n",
    "        \n",
    "        return {\n",
    "            'latent': latent,\n",
    "            'mono_recon': mono_recon,\n",
    "            'comp_recon': comp_recon,\n",
    "            'carry_pred': carry_pred,\n",
    "        }\n",
    "\n",
    "model = DeltaObserver(\n",
    "    mono_dim=mono_activations.shape[1], \n",
    "    comp_dim=comp_activations.shape[1], \n",
    "    latent_dim=16\n",
    ").to(device)\n",
    "\n",
    "print(f'üèóÔ∏è Delta Observer parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualize architecture\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.axis('off')\n",
    "\n",
    "# Draw components\n",
    "components = [\n",
    "    # (x, y, width, height, label, color)\n",
    "    (0.05, 0.7, 0.12, 0.15, f'Mono\\n({mono_activations.shape[1]}D)', '#e74c3c'),\n",
    "    (0.05, 0.3, 0.12, 0.15, f'Comp\\n({comp_activations.shape[1]}D)', '#3498db'),\n",
    "    (0.25, 0.7, 0.12, 0.1, 'Encoder\\n(32D)', '#9b59b6'),\n",
    "    (0.25, 0.35, 0.12, 0.1, 'Encoder\\n(32D)', '#9b59b6'),\n",
    "    (0.45, 0.45, 0.15, 0.2, 'Shared\\nLatent\\n(16D)', '#f39c12'),\n",
    "    (0.7, 0.7, 0.12, 0.1, 'Decoder', '#2ecc71'),\n",
    "    (0.7, 0.35, 0.12, 0.1, 'Decoder', '#2ecc71'),\n",
    "    (0.9, 0.7, 0.08, 0.1, f'Mono\\nRecon', '#e74c3c'),\n",
    "    (0.9, 0.35, 0.08, 0.1, f'Comp\\nRecon', '#3498db'),\n",
    "]\n",
    "\n",
    "for x, y, w, h, label, color in components:\n",
    "    rect = plt.Rectangle((x, y), w, h, facecolor=color, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "arrows = [\n",
    "    (0.17, 0.775, 0.25, 0.75),\n",
    "    (0.17, 0.375, 0.25, 0.40),\n",
    "    (0.37, 0.75, 0.45, 0.55),\n",
    "    (0.37, 0.40, 0.45, 0.55),\n",
    "    (0.60, 0.55, 0.70, 0.75),\n",
    "    (0.60, 0.55, 0.70, 0.40),\n",
    "    (0.82, 0.75, 0.90, 0.75),\n",
    "    (0.82, 0.40, 0.90, 0.40),\n",
    "]\n",
    "\n",
    "for x1, y1, x2, y2 in arrows:\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0.2, 0.95)\n",
    "ax.set_title('üèóÔ∏è Delta Observer Architecture', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_r2': [], 'val_r2': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print('üèãÔ∏è Training Delta Observer...\\n')\n",
    "\n",
    "pbar = tqdm(range(epochs), desc='Training')\n",
    "for epoch in pbar:\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds, train_targets = [], []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        mono_act = batch['mono_act'].to(device)\n",
    "        comp_act = batch['comp_act'].to(device)\n",
    "        carry_count = batch['carry_count'].to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mono_act, comp_act)\n",
    "        \n",
    "        # Losses\n",
    "        recon_loss = F.mse_loss(outputs['mono_recon'], mono_act) + F.mse_loss(outputs['comp_recon'], comp_act)\n",
    "        carry_loss = F.mse_loss(outputs['carry_pred'].squeeze(), carry_count)\n",
    "        \n",
    "        loss = recon_loss + carry_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(outputs['carry_pred'].squeeze().detach().cpu().numpy())\n",
    "        train_targets.extend(carry_count.cpu().numpy())\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_r2 = 1 - np.sum((np.array(train_preds) - np.array(train_targets))**2) / np.sum((np.array(train_targets) - np.mean(train_targets))**2)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds, val_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            mono_act = batch['mono_act'].to(device)\n",
    "            comp_act = batch['comp_act'].to(device)\n",
    "            carry_count = batch['carry_count'].to(device).float()\n",
    "            \n",
    "            outputs = model(mono_act, comp_act)\n",
    "            \n",
    "            recon_loss = F.mse_loss(outputs['mono_recon'], mono_act) + F.mse_loss(outputs['comp_recon'], comp_act)\n",
    "            carry_loss = F.mse_loss(outputs['carry_pred'].squeeze(), carry_count)\n",
    "            \n",
    "            loss = recon_loss + carry_loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            val_preds.extend(outputs['carry_pred'].squeeze().cpu().numpy())\n",
    "            val_targets.extend(carry_count.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_r2 = 1 - np.sum((np.array(val_preds) - np.array(val_targets))**2) / np.sum((np.array(val_targets) - np.mean(val_targets))**2)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(models_dir, 'delta_observer_best.pt'))\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_r2'].append(train_r2)\n",
    "    history['val_r2'].append(val_r2)\n",
    "    \n",
    "    pbar.set_postfix({'Loss': f'{val_loss:.4f}', 'R¬≤': f'{val_r2:.4f}'})\n",
    "\n",
    "print(f'\\n‚úÖ Best Val Loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualize training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['train_loss'], label='Train', color=COLORS['train'], linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Validation', color=COLORS['val'], linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('üìâ Training Loss', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# R¬≤ curves\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['train_r2'], label='Train', color=COLORS['train'], linewidth=2)\n",
    "ax2.plot(history['val_r2'], label='Validation', color=COLORS['val'], linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('R¬≤ (Carry Prediction)', fontsize=12)\n",
    "ax2.set_title('üìà Linear Accessibility (R¬≤)', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/delta_observer_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Final R¬≤ (Validation): {history[\"val_r2\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Extract and Analyze Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and extract latents\n",
    "model.load_state_dict(torch.load(os.path.join(models_dir, 'delta_observer_best.pt')))\n",
    "model.eval()\n",
    "\n",
    "full_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "all_latents = []\n",
    "all_carry = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in full_loader:\n",
    "        latent = model.encode(batch['mono_act'].to(device), batch['comp_act'].to(device))\n",
    "        all_latents.append(latent.cpu().numpy())\n",
    "        all_carry.append(batch['carry_count'].numpy())\n",
    "\n",
    "latent_space = np.concatenate(all_latents)\n",
    "carry_counts_all = np.concatenate(all_carry)\n",
    "\n",
    "print(f'üìä Latent space: {latent_space.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, silhouette_score\n",
    "\n",
    "# R¬≤ via linear regression\n",
    "reg = LinearRegression().fit(latent_space, carry_counts_all)\n",
    "r2 = r2_score(carry_counts_all, reg.predict(latent_space))\n",
    "\n",
    "# Silhouette score\n",
    "sil = silhouette_score(latent_space, carry_counts_all)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üìä DELTA OBSERVER LATENT SPACE METRICS')\n",
    "print('='*60)\n",
    "print(f'\\n   R¬≤ (Linear Accessibility):     {r2:.4f}')\n",
    "print(f'   Silhouette (Clustering):       {sil:.4f}')\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Visualize latent space\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    reducer = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    method_name = 'UMAP'\n",
    "except ImportError:\n",
    "    reducer = PCA(n_components=2, random_state=42)\n",
    "    method_name = 'PCA'\n",
    "\n",
    "latent_2d = reducer.fit_transform(latent_space)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "scatter = ax.scatter(latent_2d[:, 0], latent_2d[:, 1], c=carry_counts_all, \n",
    "                     cmap='viridis', s=50, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Carry Count', fontsize=12)\n",
    "cbar.set_ticks([0, 1, 2, 3, 4])\n",
    "\n",
    "# Add metrics box\n",
    "textstr = f'R¬≤ = {r2:.4f}\\nSilhouette = {sil:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray')\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='top', bbox=props, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel(f'{method_name} Dimension 1', fontsize=12)\n",
    "ax.set_ylabel(f'{method_name} Dimension 2', fontsize=12)\n",
    "ax.set_title('üî¨ Delta Observer Latent Space\\n(Colored by Carry Count)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/delta_latent_space.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save latent space\n",
    "np.savez(os.path.join(data_dir, 'delta_latent_umap.npz'),\n",
    "         latents=latent_space,\n",
    "         carry_counts=carry_counts_all,\n",
    "         bit_positions=np.zeros_like(carry_counts_all))  # Placeholder\n",
    "\n",
    "print(f'‚úÖ Latent space saved to {os.path.join(data_dir, \"delta_latent_umap.npz\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **R¬≤ (Linear Accessibility)** | ~0.95 | Semantic info is linearly accessible |\n",
    "| **Silhouette (Clustering)** | ~0.03 | No geometric clustering |\n",
    "\n",
    "**Key Finding:** The post-hoc Delta Observer achieves high linear accessibility but lower than the Online Observer (0.9505 vs 0.9879).\n",
    "\n",
    "**Why?** The Online Observer captures temporal information during training that post-hoc analysis misses!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Continue to **`03_analysis_visualization.ipynb`** for deeper geometric analysis.\n",
    "\n",
    "| Notebook | Description | Colab |\n",
    "|----------|-------------|-------|\n",
    "| **03_analysis_visualization** | Geometric analysis | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EntroMorphic/delta-observer/blob/main/notebooks/03_analysis_visualization.ipynb) |\n",
    "| **99_full_reproduction** | Complete Online pipeline | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EntroMorphic/delta-observer/blob/main/notebooks/99_full_reproduction.ipynb) |\n",
    "\n",
    "---\n",
    "\n",
    "**For Science!** üî¨üåä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
