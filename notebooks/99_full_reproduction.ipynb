{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete End-to-End Reproduction\n",
    "\n",
    "This notebook reproduces all results from the paper:\n",
    "\n",
    "> **\"Delta Observer: Learning Continuous Semantic Manifolds Between Neural Network Representations\"**  \n",
    "> Aaron (Tripp) Josserand-Austin | EntroMorphic Research Team  \n",
    "> [OSF MetaArXiv](https://doi.org/10.17605/OSF.IO/CNJTP)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Discovery: Transient Clustering\n",
    "\n",
    "**Clustering is scaffolding, not structure.** Networks build geometric organization to *learn* semantic concepts, then discard that organization once the concepts are encoded in the weights.\n",
    "\n",
    "| Training Phase | R¬≤ | Silhouette | Interpretation |\n",
    "|----------------|-----|-----------|----------------|\n",
    "| Early (epoch 0) | 0.36 | -0.02 | Random initialization |\n",
    "| Learning (epoch 20) | 0.94 | **0.33** | Clustering emerges |\n",
    "| Final (epoch 200) | 0.99 | -0.02 | Clustering dissolves |\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "1. **Generate Dataset** - All 512 possible 4-bit + 4-bit additions\n",
    "2. **Online Training** - Train all three models concurrently (Monolithic, Compositional, Delta Observer)\n",
    "3. **Trajectory Analysis** - Track R¬≤ and Silhouette during training\n",
    "4. **Discover Transient Clustering** - Observe clustering emerge then dissolve\n",
    "5. **Generate Figures** - Reproduce paper visualizations\n",
    "\n",
    "**Estimated runtime:** ~15 minutes on CPU, ~5 minutes on GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Try UMAP, fall back to PCA\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"UMAP not available, using PCA for visualization\")\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 42\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "LATENT_DIM = 16\n",
    "SNAPSHOT_INTERVAL = 5  # Save latent space every N epochs\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Training for {EPOCHS} epochs\")\n",
    "print(f\"Snapshot interval: every {SNAPSHOT_INTERVAL} epochs\")\n",
    "print(\"Configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate 4-bit Addition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_4bit_addition_dataset():\n",
    "    \"\"\"Generate all 512 possible 4-bit + 4-bit additions.\"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    carry_counts = []\n",
    "    \n",
    "    for a in range(16):\n",
    "        for b in range(16):\n",
    "            # Input: [a0, a1, a2, a3, b0, b1, b2, b3]\n",
    "            a_bits = [(a >> i) & 1 for i in range(4)]\n",
    "            b_bits = [(b >> i) & 1 for i in range(4)]\n",
    "            input_bits = a_bits + b_bits\n",
    "            \n",
    "            # Output: 5-bit sum\n",
    "            sum_val = a + b\n",
    "            output_bits = [(sum_val >> i) & 1 for i in range(5)]\n",
    "            \n",
    "            # Carry count\n",
    "            carry = 0\n",
    "            count = 0\n",
    "            for i in range(4):\n",
    "                bit_sum = a_bits[i] + b_bits[i] + carry\n",
    "                if bit_sum >= 2:\n",
    "                    count += 1\n",
    "                    carry = 1\n",
    "                else:\n",
    "                    carry = 0\n",
    "            \n",
    "            inputs.append(input_bits)\n",
    "            outputs.append(output_bits)\n",
    "            carry_counts.append(count)\n",
    "    \n",
    "    return (np.array(inputs, dtype=np.float32), \n",
    "            np.array(outputs, dtype=np.float32),\n",
    "            np.array(carry_counts, dtype=np.int64))\n",
    "\n",
    "X, y, carry_counts = generate_4bit_addition_dataset()\n",
    "print(f\"Dataset: {X.shape[0]} examples\")\n",
    "print(f\"Input: {X.shape[1]} bits, Output: {y.shape[1]} bits\")\n",
    "print(f\"Carry count distribution: {np.bincount(carry_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, X, y, carry_counts):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.carry_counts = torch.tensor(carry_counts, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.carry_counts[idx]\n",
    "\n",
    "dataset = AdditionDataset(X, y, carry_counts)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "print(\"Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonolithicMLP(nn.Module):\n",
    "    \"\"\"Standard MLP that processes all bits at once.\"\"\"\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 5)\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        hidden = torch.relu(self.fc2(x))\n",
    "        out = torch.sigmoid(self.fc3(hidden))\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "class CompositionalNetwork(nn.Module):\n",
    "    \"\"\"Modular network with separate per-bit processing.\"\"\"\n",
    "    def __init__(self, module_dim=16):\n",
    "        super().__init__()\n",
    "        self.bit_modules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(3, module_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(module_dim, module_dim),\n",
    "                nn.ReLU()\n",
    "            ) for _ in range(4)\n",
    "        ])\n",
    "        self.output = nn.Linear(4 * module_dim, 5)\n",
    "        self.hidden_dim = 4 * module_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        bit_outputs = []\n",
    "        carry = torch.zeros(batch_size, 1, device=x.device)\n",
    "        \n",
    "        for i in range(4):\n",
    "            a_bit = x[:, i:i+1]\n",
    "            b_bit = x[:, i+4:i+5]\n",
    "            module_input = torch.cat([a_bit, b_bit, carry], dim=1)\n",
    "            module_output = self.bit_modules[i](module_input)\n",
    "            bit_outputs.append(module_output)\n",
    "            carry = torch.sigmoid(module_output[:, :1])\n",
    "        \n",
    "        hidden = torch.cat(bit_outputs, dim=1)\n",
    "        out = torch.sigmoid(self.output(hidden))\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "class DeltaObserver(nn.Module):\n",
    "    \"\"\"Learns shared latent space between two architectures.\"\"\"\n",
    "    def __init__(self, mono_dim=64, comp_dim=64, latent_dim=16):\n",
    "        super().__init__()\n",
    "        self.mono_encoder = nn.Sequential(\n",
    "            nn.Linear(mono_dim, 32), nn.ReLU(), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.comp_encoder = nn.Sequential(\n",
    "            nn.Linear(comp_dim, 32), nn.ReLU(), nn.Dropout(0.1)\n",
    "        )\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(32, latent_dim)\n",
    "        )\n",
    "        self.mono_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, mono_dim)\n",
    "        )\n",
    "        self.comp_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, comp_dim)\n",
    "        )\n",
    "        self.carry_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8), nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, mono_act, comp_act):\n",
    "        mono_enc = self.mono_encoder(mono_act)\n",
    "        comp_enc = self.comp_encoder(comp_act)\n",
    "        joint = torch.cat([mono_enc, comp_enc], dim=-1)\n",
    "        return self.shared_encoder(joint)\n",
    "    \n",
    "    def forward(self, mono_act, comp_act):\n",
    "        latent = self.encode(mono_act, comp_act)\n",
    "        return {\n",
    "            'latent': latent,\n",
    "            'mono_recon': self.mono_decoder(latent),\n",
    "            'comp_recon': self.comp_decoder(latent),\n",
    "            'carry_pred': self.carry_head(latent)\n",
    "        }\n",
    "\n",
    "print(\"Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Online Training (All Models Concurrently)\n",
    "\n",
    "**This is the key innovation.** The Delta Observer watches training as it happens, capturing temporal dynamics invisible to post-hoc analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "mono_model = MonolithicMLP(hidden_dim=64).to(DEVICE)\n",
    "comp_model = CompositionalNetwork(module_dim=16).to(DEVICE)\n",
    "delta_model = DeltaObserver(mono_dim=64, comp_dim=64, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "mono_opt = optim.Adam(mono_model.parameters(), lr=LEARNING_RATE)\n",
    "comp_opt = optim.Adam(comp_model.parameters(), lr=LEARNING_RATE)\n",
    "delta_opt = optim.Adam(delta_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Trajectory storage\n",
    "trajectory = {\n",
    "    'epochs': [],\n",
    "    'latents': [],\n",
    "    'carry_counts': [],\n",
    "    'mono_acc': [],\n",
    "    'comp_acc': [],\n",
    "    'r2': [],\n",
    "    'silhouette': []\n",
    "}\n",
    "\n",
    "print(\"Models initialized\")\n",
    "print(f\"Monolithic: {sum(p.numel() for p in mono_model.parameters())} params\")\n",
    "print(f\"Compositional: {sum(p.numel() for p in comp_model.parameters())} params\")\n",
    "print(f\"Delta Observer: {sum(p.numel() for p in delta_model.parameters())} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(latents, carry_counts):\n",
    "    \"\"\"Compute R¬≤ and Silhouette score.\"\"\"\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(latents, carry_counts)\n",
    "    r2 = r2_score(carry_counts, reg.predict(latents))\n",
    "    \n",
    "    try:\n",
    "        sil = silhouette_score(latents, carry_counts)\n",
    "    except:\n",
    "        sil = 0.0\n",
    "    \n",
    "    return r2, sil\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"Compute model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _ in loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs, _ = model(inputs)\n",
    "            pred_bits = (outputs > 0.5).float()\n",
    "            correct += (pred_bits == targets).all(dim=1).sum().item()\n",
    "            total += inputs.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def snapshot_latents(mono_model, comp_model, delta_model, loader, device):\n",
    "    \"\"\"Extract latent representations for all samples.\"\"\"\n",
    "    mono_model.eval()\n",
    "    comp_model.eval()\n",
    "    delta_model.eval()\n",
    "    \n",
    "    all_latents = []\n",
    "    all_carry = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _, carry in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _, mono_h = mono_model(inputs)\n",
    "            _, comp_h = comp_model(inputs)\n",
    "            latent = delta_model.encode(mono_h, comp_h)\n",
    "            all_latents.append(latent.cpu().numpy())\n",
    "            all_carry.append(carry.numpy())\n",
    "    \n",
    "    return np.concatenate(all_latents), np.concatenate(all_carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ONLINE TRAINING - All models train concurrently\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThe Delta Observer watches training as it happens...\\n\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Training\"):\n",
    "    mono_model.train()\n",
    "    comp_model.train()\n",
    "    delta_model.train()\n",
    "    \n",
    "    for inputs, targets, carry in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        carry = carry.to(DEVICE).float()\n",
    "        \n",
    "        # --- Train Monolithic ---\n",
    "        mono_opt.zero_grad()\n",
    "        mono_out, mono_h = mono_model(inputs)\n",
    "        mono_loss = criterion(mono_out, targets)\n",
    "        mono_loss.backward()\n",
    "        mono_opt.step()\n",
    "        \n",
    "        # --- Train Compositional ---\n",
    "        comp_opt.zero_grad()\n",
    "        comp_out, comp_h = comp_model(inputs)\n",
    "        comp_loss = criterion(comp_out, targets)\n",
    "        comp_loss.backward()\n",
    "        comp_opt.step()\n",
    "        \n",
    "        # --- Train Delta Observer (detached activations) ---\n",
    "        with torch.no_grad():\n",
    "            _, mono_h_det = mono_model(inputs)\n",
    "            _, comp_h_det = comp_model(inputs)\n",
    "        \n",
    "        delta_opt.zero_grad()\n",
    "        delta_out = delta_model(mono_h_det.detach(), comp_h_det.detach())\n",
    "        \n",
    "        recon_loss = (nn.functional.mse_loss(delta_out['mono_recon'], mono_h_det.detach()) +\n",
    "                      nn.functional.mse_loss(delta_out['comp_recon'], comp_h_det.detach()))\n",
    "        carry_loss = nn.functional.mse_loss(delta_out['carry_pred'].squeeze(), carry)\n",
    "        delta_loss = recon_loss + 0.1 * carry_loss\n",
    "        delta_loss.backward()\n",
    "        delta_opt.step()\n",
    "    \n",
    "    # --- Snapshot at intervals ---\n",
    "    if epoch % SNAPSHOT_INTERVAL == 0 or epoch == EPOCHS - 1:\n",
    "        latents, carries = snapshot_latents(mono_model, comp_model, delta_model, full_loader, DEVICE)\n",
    "        r2, sil = compute_metrics(latents, carries)\n",
    "        mono_acc = compute_accuracy(mono_model, full_loader)\n",
    "        comp_acc = compute_accuracy(comp_model, full_loader)\n",
    "        \n",
    "        trajectory['epochs'].append(epoch)\n",
    "        trajectory['latents'].append(latents.copy())\n",
    "        trajectory['carry_counts'].append(carries.copy())\n",
    "        trajectory['r2'].append(r2)\n",
    "        trajectory['silhouette'].append(sil)\n",
    "        trajectory['mono_acc'].append(mono_acc)\n",
    "        trajectory['comp_acc'].append(comp_acc)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"\\nEpoch {epoch:3d}: R¬≤={r2:.4f}, Sil={sil:.4f}, Mono={mono_acc:.1f}%, Comp={comp_acc:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Online training complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Discover Transient Clustering\n",
    "\n",
    "**The key finding:** Clustering peaks during training then dissolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.array(trajectory['epochs'])\n",
    "r2_values = np.array(trajectory['r2'])\n",
    "sil_values = np.array(trajectory['silhouette'])\n",
    "\n",
    "# Find peak clustering\n",
    "peak_idx = np.argmax(sil_values)\n",
    "peak_epoch = epochs[peak_idx]\n",
    "peak_sil = sil_values[peak_idx]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSIENT CLUSTERING DISCOVERY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPeak clustering: Silhouette = {peak_sil:.4f} at epoch {peak_epoch}\")\n",
    "print(f\"Final state:     Silhouette = {sil_values[-1]:.4f} at epoch {epochs[-1]}\")\n",
    "print(f\"\\nFinal R¬≤ (accessibility): {r2_values[-1]:.4f}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"-\"*70)\n",
    "print(\"\\nClustering EMERGES during learning (scaffolding)\")\n",
    "print(\"Clustering DISSOLVES after convergence (scaffolding removed)\")\n",
    "print(\"\\nThe semantic primitive is in the TRAJECTORY, not the final state.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Transient Clustering\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "color1 = '#2ecc71'  # Green for R¬≤\n",
    "color2 = '#e74c3c'  # Red for Silhouette\n",
    "\n",
    "ax1.set_xlabel('Training Epoch', fontsize=12)\n",
    "ax1.set_ylabel('R¬≤ (Linear Accessibility)', color=color1, fontsize=12)\n",
    "line1, = ax1.plot(epochs, r2_values, color=color1, linewidth=2.5, marker='o', markersize=4, label='R¬≤')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "ax1.axhline(y=0.9, color=color1, linestyle='--', alpha=0.3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Silhouette Score (Clustering)', color=color2, fontsize=12)\n",
    "line2, = ax2.plot(epochs, sil_values, color=color2, linewidth=2.5, marker='s', markersize=4, label='Silhouette')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "ax2.set_ylim(-0.1, 0.5)\n",
    "ax2.axhline(y=0, color=color2, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Annotate peak\n",
    "ax2.annotate(f'Peak: {peak_sil:.2f}\\n(epoch {peak_epoch})',\n",
    "             xy=(peak_epoch, peak_sil),\n",
    "             xytext=(peak_epoch + 30, peak_sil + 0.08),\n",
    "             fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', color=color2, alpha=0.7),\n",
    "             color=color2)\n",
    "\n",
    "ax1.set_title('Transient Clustering: Scaffolding Emerges Then Dissolves', fontsize=14, fontweight='bold')\n",
    "ax1.legend([line1, line2], ['R¬≤ (Accessibility)', 'Silhouette (Clustering)'], loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/figure5_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Figure 5 (Training Curves) saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Final Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final latent space\n",
    "final_latents = trajectory['latents'][-1]\n",
    "final_carry = trajectory['carry_counts'][-1]\n",
    "\n",
    "# Dimensionality reduction\n",
    "if HAS_UMAP:\n",
    "    reducer = UMAP(n_components=2, random_state=RANDOM_SEED, n_neighbors=15, min_dist=0.1)\n",
    "    latents_2d = reducer.fit_transform(final_latents)\n",
    "    method = \"UMAP\"\n",
    "else:\n",
    "    reducer = PCA(n_components=2, random_state=RANDOM_SEED)\n",
    "    latents_2d = reducer.fit_transform(final_latents)\n",
    "    method = \"PCA\"\n",
    "\n",
    "# Final metrics\n",
    "final_r2, final_sil = compute_metrics(final_latents, final_carry)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(latents_2d[:, 0], latents_2d[:, 1],\n",
    "                     c=final_carry, cmap='viridis',\n",
    "                     s=50, alpha=0.7, edgecolors='white', linewidth=0.5)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, label='Carry Count')\n",
    "cbar.set_ticks([0, 1, 2, 3, 4])\n",
    "\n",
    "ax.set_xlabel(f'{method} Dimension 1', fontsize=12)\n",
    "ax.set_ylabel(f'{method} Dimension 2', fontsize=12)\n",
    "ax.set_title('Online Delta Observer Latent Space\\n(Final State)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add metrics\n",
    "textstr = f'R¬≤ = {final_r2:.4f}\\nSilhouette = {final_sil:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/figure2_delta_latent_space.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Figure 2 (Latent Space) saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations for PCA baseline comparison\n",
    "mono_model.eval()\n",
    "comp_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_inputs = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "    _, mono_h = mono_model(all_inputs)\n",
    "    _, comp_h = comp_model(all_inputs)\n",
    "    mono_act = mono_h.cpu().numpy()\n",
    "    comp_act = comp_h.cpu().numpy()\n",
    "\n",
    "# PCA baseline\n",
    "combined = np.concatenate([mono_act, comp_act], axis=1)\n",
    "pca = PCA(n_components=LATENT_DIM, random_state=RANDOM_SEED)\n",
    "pca_latents = pca.fit_transform(combined)\n",
    "pca_r2, pca_sil = compute_metrics(pca_latents, carry_counts)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"METHOD COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<25} {'R¬≤':>10} {'Silhouette':>12} {'Œî vs PCA':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Online Observer':<25} {final_r2:>10.4f} {final_sil:>12.4f} {(final_r2-pca_r2)*100:>+9.1f}%\")\n",
    "print(f\"{'PCA Baseline':<25} {pca_r2:>10.4f} {pca_sil:>12.4f} {'---':>10}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "methods = ['Online\\nObserver', 'PCA\\nBaseline']\n",
    "r2_vals = [final_r2, pca_r2]\n",
    "colors = ['#3498db', '#95a5a6']\n",
    "\n",
    "bars = ax.bar(methods, r2_vals, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for bar, val in zip(bars, r2_vals):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('R¬≤ (Linear Accessibility)', fontsize=12)\n",
    "ax.set_title('Online Observation vs PCA Baseline', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0.9, 1.01)\n",
    "ax.axhline(y=pca_r2, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Delta annotation\n",
    "delta_pct = (final_r2 - pca_r2) * 100\n",
    "ax.text(0.5, 0.95, f'+{delta_pct:.1f}%', ha='center', fontsize=14, fontweight='bold', color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/figure_method_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Method comparison figure saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trajectory data\n",
    "np.savez('../data/online_observer_trajectory.npz',\n",
    "         snapshots=np.array(trajectory['latents']),\n",
    "         epochs=np.array(trajectory['epochs']))\n",
    "\n",
    "# Save final latents\n",
    "np.savez('../data/online_observer_latents.npz',\n",
    "         latents=final_latents,\n",
    "         carry_counts=final_carry,\n",
    "         mono_activations=mono_act,\n",
    "         comp_activations=comp_act,\n",
    "         bit_positions=np.zeros_like(final_carry))  # placeholder\n",
    "\n",
    "print(\"‚úÖ Data saved to ../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REPRODUCTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä MODELS TRAINED (Online, Concurrently)\")\n",
    "print(f\"  Monolithic MLP: {trajectory['mono_acc'][-1]:.1f}% accuracy\")\n",
    "print(f\"  Compositional Network: {trajectory['comp_acc'][-1]:.1f}% accuracy\")\n",
    "\n",
    "print(\"\\nüéØ KEY DISCOVERY: TRANSIENT CLUSTERING\")\n",
    "print(f\"  Peak clustering: Silhouette = {peak_sil:.4f} at epoch {peak_epoch}\")\n",
    "print(f\"  Final state:     Silhouette = {final_sil:.4f}\")\n",
    "print(f\"  Final R¬≤:        {final_r2:.4f}\")\n",
    "\n",
    "print(\"\\nüìà METHOD COMPARISON\")\n",
    "print(f\"  Online Observer: R¬≤ = {final_r2:.4f}\")\n",
    "print(f\"  PCA Baseline:    R¬≤ = {pca_r2:.4f}\")\n",
    "print(f\"  Improvement:     +{(final_r2-pca_r2)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüìÅ FILES GENERATED\")\n",
    "print(\"  Data:\")\n",
    "print(\"    - data/online_observer_trajectory.npz\")\n",
    "print(\"    - data/online_observer_latents.npz\")\n",
    "print(\"  Figures:\")\n",
    "print(\"    - figures/figure2_delta_latent_space.png\")\n",
    "print(\"    - figures/figure5_training_curves.png\")\n",
    "print(\"    - figures/figure_method_comparison.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n  CLUSTERING IS SCAFFOLDING, NOT STRUCTURE.\")\n",
    "print(\"\\n  Networks build geometric organization to LEARN,\")\n",
    "print(\"  then DISCARD it once concepts are encoded in weights.\")\n",
    "print(\"\\n  The semantic primitive is in the TRAJECTORY,\")\n",
    "print(\"  not the final representation.\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\n‚úÖ All results successfully reproduced!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
